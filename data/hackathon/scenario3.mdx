---
title: Hybrid Cloud AI model deployment
exercise: 3
date: '2024-06-05'
tags: ['openshift','ai','kubernetes']
draft: false
authors: ['default']
summary: "Let's deploy the first model across the hybrid cloud."
---

As a sales team you've got an upcoming demo with the ACME Financial Services data science team, who have been training models on their laptops.
The team have given you access to one of their models in the ACME Financial Services object storage and want to see how this could be deployed to a cluster running in the cloud.


## 3.1 - Replicate Model to Cloud Storage

For this task, your team are required to use the `granite-7b-lab` model available in the object storage running in the ACME Financial Services on prem cluster which is based on Minio.

After locating the model in on premises object storage, your team need to replicate this model to the ACME Financial Services cloud cluster object storage so that it could be served in future.

Documentation you may find helpful is:
- https://min.io/docs/minio/linux/index.html



## 3.2 - Install Openshift AI related operators

Now that you've helped the ACME team replicate their chosen model to their cloud OpenShift Cluster, they want to serve the model ASAP.

For this challenge your team must demonstrate to ACME how to install OpenShift AI, and serve the existing model called `granite-7b-lab` via OpenShift AI.

Install the following opertors (do not install any custom resources)
- OpenShift AI
- OpenShift Service Mesh
- OpenShift Serverless

## 3.2 - Install OpenShift AI
Wait until the three operators specified in the previoius section have fully provisioned, before proceeding.
You won't need any Custom Resources for OpenShift Service Mesh and OpenShift Serverless

You will need one for OpenShift AI. A valid strategy would be to open the yaml view and go with all the defaults - the only addition to be to add this knative-serving-cert secret
        ingressGateway:
          certificate:
            `secretName: knative-serving-cert`

Documentation you may find helpful is:
- https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html/installing_and_uninstalling_openshift_ai_self-managed/index


## 3.3 - Use OpenShift AI to push Granite model from On Prem object storage to cloud

Examine your On Prem object storage (Minio). The Granite model files inside the *models* bucket represent an LLM that may have been 
- downloaded as is
- downloaded and then fine tuned.
As it's inside object storage, it's ready to push to cloud.

Now open OpenShift AI and do the following
- create a project
- create a workbench that 
  - uses Pytorch as a basis
  - uses a Persistent Volume of at least 60GB 
  - uses a Data Connection to your Minio object storage
  - uses a Medium sized Container without an accelerator
- for this task, use a Jupyter notebook that pulls the contents of your source bucket to your target bucket on the cloud (Hint available below)

Documentation you may find helpful is:

- https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html/getting_started_with_red_hat_openshift_ai_self-managed/creating-a-data-science-project_get-started

- https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html/getting_started_with_red_hat_openshift_ai_self-managed/creating-a-project-workbench_get-started


## 3.4 - Use your cloud-based OpenShift AI to Serve the model and make it easily consumable by intelligent applications for inference

### 3.4.1 - Import a VLLM Server and enable Single Model Serving
VLLM is a popular model server format whose APIs are compatible with Open AI (Chat GPT) APIs. This format then lends itself to easy migration of apps already using Open AI - to OpenShift AI.

Single Model Serving is the preferred mode for serving LLMs

Documentation you may find helpful is:
- https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html-single/serving_models/index#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models

- https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html-single/serving_models/index#deploying-models-using-the-single-model-serving-platform_serving-large-models

HINTS
[1] Use this notebook to pull from local object storage and push to cloud object storage
    https://github.com/tnscorcoran/rhods-finetunning-demo/blob/main/minio_pull_from_and_push_to.ipynb
    TODO - Code the notebook and move it to the correct git repo
    (note in a production environment, this would likely be automed using Gitops)
    TODO - confirm Gitops would be used

[2] You can import this yaml to set up your vLLM server

  TODO correct location
  https://github.com/tnscorcoran/hackathon/blob/main/temp/scenario3_hybrid_cloud/vllm-runtime-small-for-granite-7b.yaml






- 